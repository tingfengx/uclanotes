\documentclass[11pt, letter]{book}

\usepackage[margin=1.1in]{geometry}
\usepackage{amssymb}
\usepackage[
    xetex, 
    dvipsnames
]{xcolor}
\usepackage[
    colorlinks=true,
    linkcolor=black,
    urlcolor=Thistle
]{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{xargs}
\usepackage{ccicons}
\usepackage{mdframed}
\usepackage{caption}
\usepackage{cancel}
\usepackage{parskip}
\usepackage[nottoc]{tocbibind}
%\usepackage[
%    outputdir=.texpadtmp
%]{minted}

% ==== License =====
\usepackage[
    type={CC}, 
    modifier={by-nc-sa}, 
    version={4.0},
]{doclicense}

% ==== set font ====
\usepackage{amsmath}
\usepackage{unicode-math}
%\setmainfont{texgyrepagella-regular.otf}
\setmainfont{Palatino}
\setmathfont{texgyrepagella-math.otf}

% ==== todo notes ====
\usepackage[
    colorinlistoftodos,
    prependcaption,
    textsize=tiny
]{todonotes}
\newcommandx{\note}[2][1=]{\todo[linecolor=Thistle,backgroundcolor=Thistle!25,bordercolor=Thistle,#1]{#2}}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}

% General
\newcommand{\mc}[1]{\mathcal{#1}}

% Math Bold Font, Vector Notations
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bzero}{\mathbf{0}}

% Proofs, Structures
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\qed}{\hfill $\blacksquare$}

% Number Spaces, Vector Space
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\largeover}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\mathrm{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\independent}{\perp \!\!\! \perp}

\newcommand{\mods}{\mathrm{Mods}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\ent}{\mathrm{ENT}}
\newcommand{\mi}{\mathrm{MI}}
\newcommand{\dsep}{\mathrm{dsep}}

\mdfdefinestyle{theoremstyle}{%
	linecolor=Thistle!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=Thistle!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=theoremstyle]{theorem}{Theorem}[chapter]

\mdfdefinestyle{corollarystyle}{%
	linecolor=gray!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=gray!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=corollarystyle]{corollary}{Corollary}[theorem]

\mdfdefinestyle{algorithmstyle}{%
	linecolor=Orchid!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=Orchid!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=algorithmstyle]{algorithm}{Algorithm}[chapter]

\mdfdefinestyle{propositionstyle}{%
	linecolor=black!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=black!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=propositionstyle]{proposition}{Proposition}[chapter]

\mdfdefinestyle{definitionstyle}{%
	linecolor=BlueViolet!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=BlueViolet!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=definitionstyle]{definition}{Definition}[chapter]

\mdfdefinestyle{lemmastyle}{%
	linecolor=orange!20,linewidth=1pt,%
	frametitlerule=true,%
	frametitlebackgroundcolor=orange!20,
	innertopmargin=\topskip,
}
\mdtheorem[style=lemmastyle]{lemma}{Lemma}[chapter]

\newcommand*\tcircle[1]{%
  \raisebox{-0.5pt}{%
    \textcircled{#1}%
  }%
}

% Set section number in front of equation enumerations
\counterwithin{equation}{section}
\counterwithin{footnote}{section}

\author{Tingfeng Xia}
\title{CS289a: Great Theory Hits of 21st Century}
\date{Winter 2023}

\begin{document}
\maketitle

\vspace*{\fill}
Notes reorganized from \url{https://hackmd.io/@raghum/greathits}.\newline \newline 
\doclicenseThis

\tableofcontents

\chapter{Undirected s-t Connectedness}

\section{Computing Resources}
Four main computing resources that we consider as limited (and measure the performance of our algorithms against)
\begin{itemize}
	\item Time 
	\item Memory
	\item Randomness
	\item Communication
\end{itemize}

\section{Problem Statement}

\begin{itemize}
	\item \textbf{Input}: Graph $G = (V, E)$; with source and target marked as $s, t$
	\item \textbf{Output}: YES iff $s$ and $t$ are connected, NO otw.
\end{itemize}

Above is the ``traditional'' definition of $s-t$ connectivity which we can solve with a vanilla BFS or DFS. This will take $\mathcal O ( |V| + |E| ) $ and $\mathcal O (|V| )$ extra bits of space / memory. The question is then, can we solve the same problem with sub-linear extra memory usage. 

\begin{proposition}
	There is a randomized algorithm with $5 \log |V|$ bits of additional memory (directed and undirected graphs). 
\end{proposition}

\begin{proposition}[Omer Reingold, 2005]
	There is a log space ($\mathcal O ( \log | V | )$) algorithm \textbf{(deterministic)} for undirected graphs. \footnote{first great hit ...}
\end{proposition}

It is yet unknown if we can achieve log space for directed graphs (with deterministic algorithm). The best known algorithms runs with $\mathcal O (\log |V| ) ^{3/2}$ bits of memory. Why is this so challenging? 

\begin{proposition}
	If divided $s-t$ connectivity can be solved with $\mathcal O (\log |V| )$ extra bits of memory (without randomness), then any randomized algorithm can be made deterministic at the expenses of a constant factor increase in memory. 
\end{proposition}

\section{Randomized Algorithm for Connectivity}
\begin{algorithm}[Random Walk Algorithm for Connectivity] Here is the algorithm
	\begin{itemize}
		\item $steps \gets 0 $ 
		\item $current \gets s$; $target \gets t$
		\item while $steps < T$
		\begin{itemize}
			\item $current \gets $ random neighbor of current
			\item if $current == target$ return $YES$
		\end{itemize}
		\item return $NO$
	\end{itemize}
\end{algorithm}
The total memory for this algorithm is 
\begin{equation}
	2 \log N + \log T \leq 5 \log N
\end{equation}
extra bits, assuming we can get random neighbor. 

\begin{proposition}[Alenilaus, 80s]
	If $T = 100N^3$ steps, then $Pr$[Algorithm wrong] < $\frac{1}{3}$
\end{proposition}
which can improved to arbitrary accuracy by repeating the algorithm. Algorithms of this nature can perform bad on graphs known as ``Lollipop Graphs'' and even worse a ``Dumbell Graph''


\section{Log Space USTCON}
Here we highlight the progression in space complexity in various papers
\begin{itemize}
	\item \textbf{Nisan, 92}: Space $\mathcal O (\log ^ 2 N)$, time $N ^{\mathcal O (1)}$ algorithm... improved to $\mathcal O (\log ^ {4/3} N)$ in space. 
	\item \textbf{Reingold, 05}: Space $\mathcal O (\log N)$, time $N ^{\mathcal O (1)}$ algorithm. 
	\item \textbf{Trifornov, 05}: Space $\mathcal O ((\log N )(\log\log N))$ algorithm. 
\end{itemize}


\section{Spectral Graph Theory}
Consider an undirected graph $G = (V, E)$, 
\begin{definition}[Degree]
	Degree of a vertex $v$ is the number f edges $v$ is connected to.
\end{definition}

\begin{definition}[Regular]
	Graphs is ``regular'' if all vertices have same degree. 
\end{definition}

\begin{definition}[Adjacency Matrix]
	$A(G)$ is a symmetric matrix where $A(G)_{ij}$ = 1 if $\{i, j\}$ is an edge, 0 otw. 
\end{definition}

\begin{definition}[Normalized Adj Matrix]
	If $G$ is regular and has degree $D$, then the normalized adjacency matrix is defined as 
	\begin{equation}
		M(G) \equiv \frac{A(G)}{D}
	\end{equation}
\end{definition}

\begin{lemma}
	If $G$ is regular, then $1$ is an eigenvalue of $M(G)$. And $\bv_1 = \begin{bmatrix}
		1 & 1 & \dots & 1
	\end{bmatrix}^\top$ is an eigenvector with eigenvalue 1. 
\end{lemma}

\begin{proposition}[Eigenvalues of Regular Graphs]
	\label{prop:eigenvalues_of_regular_graphs}
	If $G$ is regular, then all eigenvalues of $M(G)$ have magnitude $\leq 1$. 
\end{proposition}

\begin{proof}
	WLOG assume $x_3$ is the largest entry in the vector $\bx$, then
	\begin{align}
		\lambda |x_3| 
		&= |M_{31}x_1 + M_{32}x_2 + ... + M_{3N}x_N| \\
		&\leq M_{31}|x_3| + M_{32}|x_3| + ... + M_{3N} |x_3| \\
		&= (M_{31} + \ldots + M_{3N}) |x_3| \\
		&= 1 |x_3|
	\end{align}
	Thus, $\lambda \leq 1$. \qed
\end{proof}

\begin{proposition}[Connectedness and Matrices]
	Regular $G = (V, E)$ is connected if and only if the only eigenvector with eigenvalue 1 for $M(G)$ is the all 1 vector. \footnote{i.e., eigenvalue 1 has an multiplicity of 1.}
\end{proposition}

\begin{proof} [Regular $G = (V, E)$ is connected implies $\lambda = 1$ has multiplicity of 1 for $M(G)$.]
	From proof to Prop. \ref{prop:eigenvalues_of_regular_graphs} we already know that $|\lambda| \leq 1$. With $x_j = \max(\bx)$ as the largest entry in the eigenvector, we recall (and abstract the inequality used back then as
	\begin{equation}
		|\lambda||x_j| = |(M(G)\bx)_j| = \left| \sum_{v_i \in N(v_j)} x_i \right| / D \leq |x_j| 
	\end{equation}
	We are now interested in the condition of when $\lambda = 1, |\lambda| |x_j| = |x_j|$, in which case we need 
	\begin{equation}
		x_i = x_j, \quad \forall v_i \in N(v_j)
	\end{equation}
	This suffices as a proof to every eigenvector with eigenvalue 1 to $M(G)$ is the $\mathbf 1$ vector. \qed
\end{proof}

\begin{proof} [$\lambda = 1$ has multiplicity of 1 for $M(G)$ implies regular $G = (V, E)$ is connected.]
	todo \dots
\end{proof}

\begin{proposition}[Eigenvalues of a Regular Graph]
	If $G$ is regular, then the eigenvalues of $M(G)$ are 
	\begin{equation}
		1 = \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N
	\end{equation}
\end{proposition}

\begin{proof}
	This follows from the proof for Prop. \ref{prop:eigenvalues_of_regular_graphs} where we proved that all eigenvalues of $M(G)$ have magnitude $\leq$ 1. Since the one vector $\mathbf 1$ is an eigenvector of $M(G)$ with eigenvalue one, we know that $\lambda_1 = 1$ is attainable. This suffices as a proof. \qed
\end{proof}

\begin{proposition}
	$G$ is connected and regular if and only if on $M(G)$
	\begin{equation}
		\max ( |\lambda_2| , |\lambda_3|, \dots, |\lambda_N|) \leq 1
	\end{equation}
\end{proposition}

\begin{proof}
	todo ... 
\end{proof}

\begin{proposition}[Eigenvalues of D-Regular Graphs]
	If $G$ is a D-regular graph, then
	\begin{itemize}
		\item 1 is an eigenvalue of $M(G)$, and 
		\item all eigenvalues of $M(G)$ are at most 1 in absolute value
	\end{itemize}
\end{proposition}

\begin{definition}[Self Loops]
	We add connections from each node in the graph to themselves. In the matrix representation, we set $G_{ii} = 1, \forall i$. 
\end{definition}

\begin{definition}[Second Largest Eigenvalue]
	... denoted as $\lambda(G)$ or $\lambda_2(G)$. 
\end{definition}

\begin{lemma}
	If $G$ is D-regular and \textbf{has self loops}, then $G$ is connected if and only if $\lambda(G) < 1$.
\end{lemma}

\begin{proof} 
[$G$ is disconnected implies $\lambda(G) = 1$ (via contrapositive).] 
Consider a graph $G$ such that it is comprised of two clouds of disjoint graphs $G_1$ and $G_2$. Then the adjacency matrix of $G$ will take a block matrix form
\begin{equation}
	M_G = \begin{bmatrix}
		M_{G_1} & [\bzero ]  \\
		[\bzero ] & M_{G_2}
	\end{bmatrix}
\end{equation}
From linear algebra, we know that the eigenvalues of $M_G$ will be the union of eigenvalues of $M_{G_1}$ and $M_{G_2}$. Now, consider
\begin{equation}
	\bx^{(1)} = 
	\begin{bmatrix}
		1 \\ 1 \\ \vdots \\ 0 \\ 0
	\end{bmatrix} 
	\quad \quad \text{and} \quad \quad
	\bx^{(2)} = 
	\begin{bmatrix}
		0 \\ 0 \\ \vdots \\ 1 \\ 1
	\end{bmatrix}
\end{equation}
are both eigenvectors of $M_G$ with eigenvalues of 1. Hence, there are two orthogonal eigenvectors with eigenvalue 1, and $\lambda(G) = 1$ as wanted. 
\qed

\end{proof}

\begin{proof} [If $G$ is connected, then $\lambda(G) < 1$.] We already know that 1 is an eigenvalue of $M_G$ with the $\mathbf 1$ vector as eigenvector. Suppose $\lambda$ is also an eigenvalue with $\bv$ as an eigenvector and $\bv$ is perpendicular to $\mathbf 1$. Now, 
\begin{equation}
	\label{eq:eigenvector_perp_to_1}
	\mathbf 1 \perp \bv \implies \langle \bv, \mathbf 1 \rangle = v_1 + v_2 + \dots  + v_N  = 0
\end{equation}
The vector $\bv$ must contain some positive entries and some negative entries, we separate them into two sets
\begin{equation}
	P = \{ i : v_i \geq 0\} \quad \text{and} \quad N = \{ i : v_i < 0 \}
\end{equation}
where both sets are non-empty by Eq. \ref{eq:eigenvector_perp_to_1}. Taking a step back and reorganize the goal into matrix form
\begin{equation}
	M_G \begin{bmatrix}
		+ \\ + \\ \vdots \\ - \\ -
	\end{bmatrix} = \lambda \begin{bmatrix}
		+ \\ + \\ \vdots \\ - \\ -
	\end{bmatrix}\quad \quad \text{where} \quad \begin{bmatrix}
		+ \\ + \\ \vdots \\ - \\ - \end{bmatrix} = \begin{bmatrix}
			\mathbf P \\ - \\ \mathbf N
		\end{bmatrix} = \bv
\end{equation}
Per element, 
\begin{equation}
	\sum_{j = 1}^N M_G[i, j]\cdot v_j = \lambda \cdot v_i, \quad \forall i
\end{equation}
By the connectedness assumption, there must always be some edge connecting $P$ and $N$ the two sets, so
\begin{align}
	\lambda \left( \sum_{i \in P} v_i \right) 
	&= \sum_{i \in P} \left( \sum_{j = 1}^N M_G[i, j] \cdot v_j \right) \\
	&= \sum_{j = 1}^N v_j \sum_{i \in P} M_G[i, j] \\
	&= \sum_{j \in P} v_j \left( \sum_{i \in P} M_G[i, j] \right) + \sum_{j \in N} v_j \left( \sum_{i \in P} M_G[i, j] \right) \\
	&\leq \sum_{j \in P} v_j (1) + \sum_{j \in N} v_j \left( \sum_{i \in p} M_G[i, j] \right) \\
	&< \sum_{j \in P} v_j
\end{align}
where in the last two steps we utilized the facts that $M_G$'s columns add up to 1 and we have at least 1 non-zero entry in each row and col of $M_G$. 

In summary, we obtained
\begin{equation}
	\lambda \left( \sum_{i \in P} v_i \right) < \left( \sum_{j \in P} v_j \right) \quad \implies \quad \lambda < 1
\end{equation}
\qed

\end{proof}

\begin{definition}[Spectral Gap]
	Spectral Gap of a D-regular graph G is defined as 
	\begin{equation}
		\text{Spectral Graph} 
		\equiv 1 - \lambda(G)
	\end{equation}
\end{definition}

\begin{lemma}
	If $G$ is a D-regular connected graph with self-loops, then
	\begin{equation}
		\lambda (G) \leq 1 - \frac{1}{2D^2 \cdot N^2}
	\end{equation}
\end{lemma}

\begin{definition}
	We say a graph $G$ is $(N, D, \lambda)$ if it has $N$ vertices, $D$ regular and $\lambda(G) \leq \lambda$. 
\end{definition}


\section{Path Enumeration}
The simplest case is wen the shortest path between $s, t$ is short. Then, we can enumerate all paths of some length and see if $t$ is reached. 

The algorithm goes as follows
\begin{algorithm}
	1, Explore all paths of length less than or equal to $T$ from $s$. 2, If you reach $t$ in these explorations, output YES. If not, output NO. 
\end{algorithm}
This takes $\mathcal O (\log D) \cdot T$ extra space, where $D$ is the degree of the graph and $T$ is the loop times. 

\begin{definition}[Graph Diameter]
	Diameter of a graph is defined as the length of the longest shortest path for any pair of vertices. By convention, 
	\begin{itemize}
		\item G disconnected, diameter = $\infty$, and
		\item G connected, diameter = $\max_{i \neq j} \left( ShortestPath (i, j) \right)$
	\end{itemize}
\end{definition}

\begin{proposition}[Extra Space for Path Enumeration]
	Path enumeration will solve the $s-t$ connectivity in with max extra space 
	\begin{equation}
		(\log D) \cdot \Delta (G) \label{eq:path_enumeration_extra_space}
	\end{equation} 
	bits, where $\Delta (G)$ is the max diameter of connected components of $G$. 
\end{proposition}

\begin{proposition}
	If $G$ is connected, D-regular, has self-loops, then\footnote{$\lambda$ is the second largest eigenvalue, $N$ is the matrix size (number of nodes).}
	\begin{equation}
		Diameter(G) \leq \lceil \log _{\frac{1}{\lambda}} N \rceil + 1
	\end{equation}
\end{proposition}


\subsection{Reingold's Idea}\label{reingold_algo}
We see from the proposition above that the bigger the spectral gap, the smaller the number of extra bits we need in space for the algorithm. The problem then is how we can transform the graph enlarging the spectral gap while not hurting the degree too much. Formally, we want to transform $(G, s, t)$ to $(\bar G, \bar s, \bar t)$ such that 
\begin{itemize}
	\item $s, t$ connected in $G$ if and only if $\bar s, \bar t$ connected in $\bar G$, and
	\item $\lambda (\bar G) < \lambda (G)$, and
	\item $Degree(\bar G)$ is not much worse then $Degree(G)$
\end{itemize}

\subsection{Reducing Degree} \label{sec:reducing_degree}
For the first part of Eq. \ref{eq:path_enumeration_extra_space}, we can reduce the degree of any graph with

\begin{algorithm}[Degree Reduce Procedure] The procedure, 
	\begin{itemize}
		\item Break each edge into two vertices, and 
		\item Add local edges at each ``old'' vertices, and
		\item Add self loops to make graph
	\end{itemize}
\end{algorithm}

\begin{proposition}
	The procedure outlined above generates a degree 4 graph. 
\end{proposition}


\subsection{Improving Spectral Gap}
\begin{definition}[Multi-graphs]
	A multi-graph is a superset of our old definition of a graph, except we allow repeated edges between nodes. This is represented as values larger than 1 in the adjacency matrix. All definitions are carried over without change: degree, normalized adjacency matrix, and $\lambda (G)$.
\end{definition}

With the degree reducing algorithm in Sec. \ref{eq:path_enumeration_extra_space}, we can reduce any graph to a degree of 4. This means Eq. \ref{eq:path_enumeration_extra_space} is now transformed into
\begin{equation}
	( \log 4 ) \cdot \Delta (G) = 2 \cdot \Delta (G)
\end{equation}
extra bits of storage. How should we improve
\begin{equation}
	\Delta (G) \leq \log _{\frac{1}{\lambda}} N
\end{equation}
which is the largest diameter of any connected component?

\begin{mdframed}
	\textbf{Idea:} Input $G, s, t$ where $G$ has self-loops and transform that into $G', s', t'$ where 
	\begin{equation}
		\lambda (G') \ll \lambda (G)
	\end{equation}
	\textbf{Goal:} Operations to improve (decrease) the second largest eigenvalue. 
\end{mdframed}

\begin{definition}[Squaring the Graph]
	Add new edges: if $(u, v)$ and $(v, w)$ are edges, then add an edge $(u, w)$. 
\end{definition}

\begin{proposition}[Adjacency of Squared Graph]
	\begin{equation}
		A_{G^2} = ( A_G ) ^ 2
	\end{equation}
	in matrix representation, and we allow multi-graph in this setting. 
\end{proposition}

\begin{proof}
	\begin{equation}
		(A_G)^2_{[i, j]} = \sum_{k = 1}^N (A_{G})_{[i, k]} (A_{G})_{[k, j]}
	\end{equation}
	\qed
\end{proof}

\begin{proposition}[Squared Graph Spectral Gap]
	If $G$ is a $(N, D, \lambda)$ graph with self loops, then $G^2$ is a $(N, D^2, \lambda^2)$ graph with self loop. Since connected $\implies \lambda < 0$, $\lambda^2 < \lambda$.
\end{proposition}

\begin{theorem}[Squared Matrix Spectral Decomposition]
	\label{thm:squared_matrix_spectral_decomposition}
	$M$ is a symmetric matrix with eigenvalues
	\begin{equation}
		\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \dots \geq \lambda_N
	\end{equation}
	then, $M^2$ is a symmetric matrix with the same eigenvectors but with eigenvalues
	\begin{equation}
		\lambda_1^2, \lambda_2^2, \dots, \lambda_N^2 
	\end{equation}
\end{theorem}

\begin{proof}
	For $M$, we have
	\begin{equation}
		M\bx = \lambda \bx
	\end{equation}
	Then, 
	\begin{equation}
		M^2 \bx = \lambda M\bx = \lambda^2 \bx
	\end{equation}
	This concludes the proof. \qed
\end{proof}

\begin{corollary}[Squared Graph Eigenvalues]
	It follows from Thm. \ref{thm:squared_matrix_spectral_decomposition} directly that if $\lambda_1, \dots, \lambda_N$ are eigenvalues for the original graph matrix $M$, then the new squared $M^2$ matrix has the same eigenvectors but with eigenvalues $\lambda_1^2, \lambda_2^2, \dots, \lambda_N^2 $ instead. 
\end{corollary}

\begin{proposition}[Normalized Adjacency of Squared Graph]
	The normalized graph matrix of $G^2$, is such that
	\begin{equation}
		M_{G^2} = \left ( M_G \right ) ^2
	\end{equation}
\end{proposition}

\begin{proof}
	Recall that 
	\begin{equation}
		M_G = \frac{A_G}{D}
	\end{equation}
	Then, 
	\begin{equation}
		M_{G^2} = \frac{A_{G^2}}{D^2} = \frac{(A_G)^2}{D^2} = \left( \frac{A_G}{D} \right) ^2 = (M^G) ^2
	\end{equation}
	This concludes the proof. \qed
\end{proof}

\begin{proposition}[Square Graph Does Not Save Memory]
	Recall that our initial goal was to save extra memory used. Here with squaring, though we enlarged the spectral gap as desired ($(1 - \lambda) \rightarrow (1 - \lambda^2) $, the degree got larger ($D \rightarrow D^2$). In total, extra bits is
	\begin{align}
		\left ( \log D \right ) \cdot \log_{\frac{1}{\lambda}} N
		&\leadsto \left( \log D^2 \right) \log _{\frac{1}{\lambda^2}} N \\
		&= 2 \cdot \log D \cdot \frac{1}{2} \cdot \log _{\frac{1}{\lambda}} N \\
		&= \left ( \log D \right ) \cdot \log_{\frac{1}{\lambda}} N
	\end{align}
	which is exactly what we had before. This suffices as a proof for squaring matrices alone does not bring any memory savings. \qed
\end{proposition}

\paragraph{Goal} Taking a step back, we can see that we need to find a powering operation that improves the second largest eigenvalue \textbf{while not increasing degree too much}. This leads to the following algorithm:

\begin{algorithm}[Reingold, 2005]
	For a graph specified as $(G, s, t)$ where $G$ is 4-regular and has self-loops, define a recursive relationship 
	\begin{equation}
		G_{i + 1} = G_i ^2 \tcircle{z} H
	\end{equation}
	where $H$ is a special graph. This recursion covers the transformation
	\begin{equation}
		(G, s, t) \leadsto (G_1 = G^2 \tcircle{z} H, \bar s, \bar t) \leadsto (G_2 = G_1^2 \tcircle{z} H, \bar{\bar{s}}, \bar{\bar{t}} ) \leadsto \dots
	\end{equation}
	
	\paragraph{Remark} $G_i^2$ part decreases the second largest eigenvalue, and the $\tcircle{z} H$ part brings down the degree while not hurting second largest eigenvalue. 
\end{algorithm}

\begin{definition}[Consistent Labelling]
	$G$ is a D-regular graph. A consistent labelling is a mapping 
	\begin{equation}
		L: \mathbb E \rightarrow [D]
	\end{equation}
	such that at each vertex all edges of the vertex have distinct labels. 
\end{definition}


\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figs/consistentlabelling-1.png}
	\caption{Illustration of consistent labelling.\label{fig:consistent-labeling-1}}
\end{figure}


\paragraph{Example} Figure \ref{fig:consistent-labeling-1} depicts a consistent edge labelling of the graph. 

\begin{definition}[Zig Zag Product]
	\textbf{Input \& Output}
	\begin{equation}
		\left. \begin{aligned}
 			G: (N, D, -) \\
 			H: (D, D_1, -) 
 		\end{aligned}\right\rbrace \rightarrow G \tcircle{z} H : (ND, D_1^2, -)
	\end{equation}
	\textbf{Rotations}
	\begin{align}
		&\left. \begin{aligned}
 			Rot_G: [N] \times [D] \rightarrow [N] \times [D] \\
 			Rot_H: [D] \times [D_1] \rightarrow [D] \times [D_1]
 		\end{aligned}\right \rbrace 
 		\\
 		&\quad \quad \quad \quad \quad \quad
 		\rightarrow Rot_{G\tcircle{z} H}: [N\cdot D] \times \left( [D_1^2] \right) \rightarrow [N\cdot D] \times \left( [D_1^2] \right)
 		\\
 		&\quad \quad \quad \quad \quad \quad
 		\equiv Rot_{G\tcircle{z} H}: [N\cdot D] \times \left( [D_1] \times [D_1] \right) \rightarrow [N\cdot D] \times \left( [D_1] \times [D_1] \right)
	\end{align}
	and 
	\begin{align}
		&Rot_{G \tcircle {z}  H} \left ( (v, a) , (k_1, k_2) \right) : \\
		&\quad \rightarrow (a', i') \gets Rot_H(a, k_1) \\
		&\quad \rightarrow (w, b') \gets Rot_G (v, a') \\
		&\quad \rightarrow (b, i'') \gets Rot_H (b', k_2) \\
		&\quad \rightarrow \text{output} \left( (w, b), (k_2, k_1) \right)
	\end{align}
	\paragraph{English Explanation} The Zig-Zag product $G \tcircle{z} H$ replaces each vertex of $G$ with a copy (cloud) of $H$, and connects the vertices by moving a small step (zig) inside the cloud, followed by a big step between two clouds, and finally performs another small step (zag) inside the destination cloud. 
\end{definition}














































\vfill 
\begin{equation*}
	\heartsuit
\end{equation*}
\end{document}