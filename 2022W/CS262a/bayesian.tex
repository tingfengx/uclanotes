\documentclass[11pt]{article}

\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[
    xetex, 
    dvipsnames
]{xcolor}
\usepackage[
    colorlinks=true,
    linkcolor=black,
    urlcolor=Thistle
]{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{xargs}
\usepackage{ccicons}
\usepackage{mdframed}
\usepackage{caption}
\usepackage{cancel}
\usepackage{float}
\usepackage[nottoc]{tocbibind}
%\usepackage[
%    outputdir=.texpadtmp
%]{minted}

% ==== License =====
\usepackage[
    type={CC}, 
    modifier={by-nc-sa}, 
    version={4.0},
]{doclicense}

% ==== set font ====
\usepackage{amsmath}
\usepackage{unicode-math}
%\setmainfont{texgyrepagella-regular.otf}
\setmainfont{Palatino}
\setmathfont{texgyrepagella-math.otf}

% ==== todo notes ====
\usepackage[
    colorinlistoftodos,
    prependcaption,
    textsize=tiny
]{todonotes}
\newcommandx{\note}[2][1=]{\todo[linecolor=Thistle,backgroundcolor=Thistle!25,bordercolor=Thistle,#1]{#2}}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}

% General
\newcommand{\mc}[1]{\mathcal{#1}}

% Math Bold Font, Vector Notations
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bzero}{\mathbf{0}}

% Proofs, Structures
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\qed}{\hfill $\blacksquare$}

% Number Spaces, Vector Space
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\largeover}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\mathrm{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\independent}{\perp \!\!\! \perp}

\newcommand{\mods}{\mathrm{Mods}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\ent}{\mathrm{ENT}}
\newcommand{\mi}{\mathrm{MI}}
\newcommand{\dsep}{\mathrm{dsep}}

% Set section number in front of equation enumerations
\counterwithin{equation}{section}
\counterwithin{footnote}{section}
\author{\ccLogo\, Tingfeng Xia @ UCLA}
\title{CS262a Bayesian Networks}
\date{Winter Quarter, 2022}

\begin{document}
\maketitle
\doclicenseThis

\section*{Preface}
This document is intentionally kept short, and is not a complete summary of what is covered in this course. Excerpt from online: 
\begin{quotation}
\noindent The objective of this class is to provide an in-depth exposition of knowledge representation, reasoning, and machine learning under uncertainty using the framework of Bayesian networks. Both theoretical underpinnings and practical considerations will be covered, with a special emphasis on constructing and learning graphical models, and on various exact and approximate inference algorithms. Additional topics include logical approaches to probabilistic inference, compilation techniques, sensitivity analysis, undirected graphical models, and statistical relational learning.
\end{quotation}

\paragraph{Instructor:} \href{https://web.cs.ucla.edu/~darwiche/}{Professor Adnan Darwiche}
\paragraph{Book:} \href{https://www.cambridge.org/us/catalogue/catalogue.asp?isbn=9780521884389}{Adnan Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press 2009.}

\tableofcontents

\newpage
\section{Propositional Logic}
\subsection{Principle Logical Forms}
\paragraph{Inconsistent.} Something that never holds; $\mods (\cdot) = \empty$; $\pr(\alpha) = 0$
\paragraph{Valid.} Something that always holds; $\mods (\cdot) = \Omega$; $\pr (\alpha) = 1$
\paragraph{Equivalent.} $\mods (\alpha) = \mods (\beta)$
\paragraph{Mutual Exclusive.} $\mods(\alpha) \cap \mods (\beta) = \emptyset$
\paragraph{Exhaustive.} $\mods (\alpha) \cup \mods (\beta) = \Omega$
\paragraph{Entailment / Implication.} $\alpha \vDash \beta \triangleq \mods (\alpha) \subseteq \mods ( \beta)$

\subsection{Equivalent Forms}
\begin{itemize}
\item $\mods( \alpha \land \beta) = \mods( \alpha) \cap \mods(\beta)$
\item $\mods( \alpha \vee \beta) = \mods ( \alpha) \cup \mods ( \beta)$
\item $\mods ( \neg \alpha) = \largeover{\mods ( \alpha)}$
\end{itemize}

\subsection{Instantiation Agreement}
Two instantiation, each of which can cover a subset of different varaibles, are said to be compatible with each other if they argree on all common variables. Denoted as $\bx \sim \by$.

\subsection{Information Theory}
\paragraph{Entropy.} 
\begin{equation}
\ent  (X) = - \sum_x \pr (x) \log \pr( x)
\end{equation}
where $0 \log 0 = 0 $ by convention. With a higher entropy, we say that it is more chaotic. 

\paragraph{Conditional Entropy. }
\begin{equation}
	\ent (X | Y ) = \sum_y \pr (y) \ent(X | y) \quad \text{where} \quad \ent (X|y) = - \sum_{x} \pr(x|y) \log \pr(x | y)
\end{equation}
Conditioning never increases the entropy, i.e. 
\begin{equation}
	\ent (X | Y) \leq \ent (X)
\end{equation}

\paragraph{Mutual Information} 
\begin{align}
	\mi(X; Y) 
	&= \sum_{x, y} \pr (x, y) \log \frac{\pr (x, y)}{\pr (x) \pr (y)} \\
	&= \ent (X) - \ent (X | Y) \\
	&= \ent (Y) - \ent ( Y | X) 
\end{align}

\paragraph{Conditional Mutual Information}
\begin{align}
	\mi ( X; Y | Z) 
	&= \sum_{x, y, z} \pr (x, y , z) \log \frac{\pr (x, y | z)}{\pr (x | z) \pr(y | z)} \\
	&= \ent (X | Z) - \ent (X | Y, Z) \\
	&= \ent (Y | Z) - \ent (Y | X, Z)
\end{align}

\section{Probability Calculus}
\subsection{Bayesian Conditioning}
Bayesian Condition is specified by the formula 
\begin{equation}
	\pr ( \alpha | \beta) = \frac{\pr ( \alpha \land \beta )}{\pr ( \beta) }
\end{equation}
In particular, not to be confused with Bayesian inference (to be added later). 

\subsection{Independence and Notations}
\paragraph{Independence.}  
\begin{align}
	\alpha \independent \beta 
	& \iff \pr ( \alpha | \beta) = \pr ( \alpha) \vee \pr( \beta) = 0 \\
	& \iff \pr ( \alpha \land \beta) = \pr(\alpha) \pr ( \beta)
\end{align}

\paragraph{Conditional Independence.}
\begin{align}
	(\alpha \independent \beta) | \gamma 
	&\iff \pr ( \alpha | \beta \land \gamma) = \pr ( \alpha | \gamma) \vee \pr(\beta \land \gamma) = 0 \\
	&\iff \pr ( \alpha \land \beta | \gamma) = \pr ( \alpha | \gamma ) \pr(\beta | \gamma ) \vee \pr ( \gamma) = 1
\end{align}

\paragraph{Set Independence}
\begin{equation}
	I_{\pr} (X, Z, Y) \iff (x \independent y) | z, \quad \forall x, y, z \in X, Y, Z
\end{equation}

\section{Bayesian Networks}
\subsection{Soft Evidence}
\subsubsection{All Things Considered Method}
We normalize / rescale $w$ according to new evidence. 
\begin{align}
\pr' (w) = \begin{cases}
	\frac{\pr ' (\beta)}{\pr (\beta)} \pr ( w) &  \text{if} w \vDash \beta \\
	\frac{\pr' (\neg \beta)}{\pr (\neg \beta)} \pr (w) & \text{if} w \vDash \neg \beta
\end{cases}
\end{align}

The closed form is called the Jefferey's Rule. 

\paragraph{Jeffery's Rule}
\begin{equation}
	\pr ' (\alpha) = q \pr ( \alpha | \beta ) + (1 - q) \pr ( \alpha | \neg \beta)
\end{equation}

\paragraph{Jeffery's Rule - General Case}
\begin{equation}
	\pr'(\alpha) = \sum_{i = 1}^n\pr ' (\beta_i) \pr( \alpha | \beta_i) 
\end{equation}

\subsubsection{Nothing-else Considered Method}
\paragraph{Odds. } 
\begin{equation}
	O(\beta) = \frac{\pr( \beta)}{\pr ( \neg \beta)}
\end{equation}

\paragraph{Bayes Factor} 
\begin{equation}
	k = \frac{O'(\beta)}{O ( \beta)} = \frac{Pr'(\beta) / \pr'(\neg \beta)}{...}
\end{equation}
from where we can expand and organize
\begin{equation}
	\pr' ( \beta) = \frac{k \pr ( \beta) }{k \pr ( \beta) + \pr ( \neg \beta) } 
\end{equation}

\paragraph{Closed Form Solution.}
\begin{equation}
	\pr' (\alpha) = \frac{k\pr ( \alpha \land \beta) + \pr (\alpha \land \neg \beta)}{k \pr ( \beta ) + \pr ( \neg \beta)}
\end{equation}

\subsection{Noisy Sensors}
\begin{equation}
	O'(\beta) = \underbrace{\frac{1 - f_n}{f_p}}_{k^+} O(\beta) \quad \quad O'(\beta) = \underbrace{\frac{f_n }{1 - f_p}}_{k^-} O(\beta)
\end{equation}

\subsection{Markov Assumptions}
\begin{equation}
	\mathrm{Markov}(G) = \{I_\pr ( V, \mathrm {Parents} (V), \mathrm{ND} (V) \}_V
\end{equation}
where $\mathrm{ND}$ means non-descendants, and includes all nodes except for $V, \mathrm {Parents} (V)$ and $\mathrm {Descendants}(V)$ (all the way till leaf)

\subsection{Graphoid Axioms\label{sec:graphoid}}
\paragraph{Symmetry.}
\begin{equation}
	I_\pr (X, Z, Y) \iff I _\pr (Y, Z, X) 
\end{equation}

\paragraph{Decomposition.}
\begin{equation}
	I_\pr (X, Z, Y \cup W) \implies I _\pr (X, Z, Y) \land I _\pr (X, Z, W)
\end{equation}

\paragraph{Weak Union.}
\begin{equation}
	I_\pr (X, Z, Y \cup W) \implies I_\pr (X, Z \cup Y, W) 
\end{equation}

\paragraph{Contraction.}
\begin{equation}
	I_\pr (X, Z, Y) \land I_\pr (X, Z \cup Y, W ) \implies I_\pr (X, Z, Y \cup W )
\end{equation}

\paragraph{Triviality.}
\begin{equation}
	I_\pr (X, Z, \emptyset) 
\end{equation}

\subsection{Positive Graphoid Axioms}
... includes everything from Graphoid Axioms (Section \ref{sec:graphoid}) and in addition has 
\paragraph{Intersection.}
\begin{equation}
	I _ \pr (X, Z \cup W, Y ) \land I _\pr (X, Z \cup Y , W ) \implies I _\pr (X, Z, Y \cup W)
\end{equation}

\subsection{D-seperation Linear Prune Theorem}

\subsection{D-seperation Properties}
\paragraph{Soundness.} 
\begin{equation}
	\dsep_ G ( X, Z, Y ) \implies I _\pr (X, Z, Y)
\end{equation}

\paragraph{(Weak) Completeness.}
There exists a parametrization $\Theta$ that for every DAG $G$ such that
\begin{equation}
	I_\pr (X, Z, Y) \iff \dsep_G (X, Z, Y)
\end{equation}

\section{Inference by Variable Elimination}
\subsection{Interaction Graphs} \todo{add to here}


\subsection{Elimination Order} Choosing an optimal elimination order in VE is NP-hard. We resort to heuristics. 

\subsubsection{Minimum Degree Ordering}
When choosing node to eliminate, choose the one that has the smallest number of neighbours in the interaction graph. 

\subsubsection{Minimum Fill Ordering}

\section{Inference by Factor Elimination}
\subsection{Factor Elimination}
The elimination of factor $f_i$ from a set of factors $S$ is a two step process. We first eliminate all variables $V$ that appear only in factor $f_i$ and then multiply the result $\sum_V f_i$ by some other factor $f_j \in S$. 

\paragraph{Projection.} Factor projection operation is defined as 
\begin{equation}
	\mathrm {project} (f, Q) \triangleq \sum_{vars(f) \setminus Q} f
\end{equation}
which is essentially summing out all variables not in $Q$. In the vanilla factor elimination algorithm (\texttt{FE1}), the projection is used in the return statement. This is essentially a normalization step (onto variables specified only).

\subsection{Elimination Trees\label{sec:elimination tree}}
\paragraph{Variables.}
$vars(i)$ denotes the variables mentioned at node $i$. $vars(i, j)$ denotes all variables mentioned in nodes to the $i$-side of the graph (inclusive). Hence, it holds that $vars(i) \subseteq vars(i, j)$.

\paragraph{Separators.} 
\begin{equation}
	S_{ij} \triangleq vars(i, j) \cap vars(j, i)
\end{equation}

\paragraph{Clusters.} 
\begin{equation}
	C_i \triangleq vars(i) \cup \bigcup_j S_{ij}
\end{equation}

\subsection{Message Passing Formulation}
The Message that $i$ sends to $j$ is 
\begin{equation}
	M_{ij} \triangleq \mathrm{project} \left( \phi_i \prod_{k \neq j } M_{ki}, S_{ij} \right) 
\end{equation}
where the product is off all the messages from edges except for the output direction and $S_{ij}$ is the separator defined in Section \ref{sec:elimination tree}.


\section{Inference by Conditioning}
\subsection{Run time Comparison}
\subsubsection{Variable Elimination (VE)}
Let $w$ be the width of the tree, $n$ denote the number of variables, and $|Q|$ as query variable size.
\paragraph{Time Complexity.} $\mathcal O (n \exp (w))$

\paragraph{Space Complexity.} 
\begin{equation}
	\mathcal O (n \exp (w) + n \exp (|Q|)) \equiv \mathcal O (n \exp (w)) \text{, \quad if } |Q| < \infty
\end{equation}

\subsubsection{Massage Passing}
\paragraph{One Specific Message.} The cost to pass one specific message is
\begin{equation}
	\mathcal O (\exp ( |C_i|)) 
\end{equation}
where $|C_i|$ is the cluster size. 

\paragraph{Every Message.} Since cluster sizes are bounded above by tree width, we can say that passing of every message will have an upper-bound runtime of
\begin{equation}
	\mathcal O ( \exp (w) )
\end{equation}
where $w$ is the width of the elimination tree. 

\paragraph{Amount of Messages.} The total amount of messages is 
\begin{equation}
	\mathcal O (2 (m - 1)) \quad \quad \text{where $m = |V|$}
\end{equation}
since we have a tree structure, meaning we have exactly $(m - 1)$ edges and each edge can have forward backward each once. 

\paragraph{All Messages - All Cluster Marginals.} The total time to pass all messages and compute all cluster marginals is 
\begin{equation}
	\mathcal O (m \exp (w)) \quad \quad \text{or} \quad \quad \mathcal O (n \exp( w))\text{, for $\mathcal O (n)$ edges.} 
\end{equation}

\subsubsection{Polytree / Belief Propagation}
\paragraph{Runtime.} Define $k$ as the max number of parents in the poly tree, then $k$ is the same as the width of elimination tree. Let, also, $n$ denote the number of nodes in the polytree. The algorithm has runtime 
\begin{equation}
	\mathcal O (n \exp (k))
\end{equation}

\subsubsection{Cut Set Conditioning}
\paragraph{Time and Space (Total) Complexity.}
\begin{equation}
	\mathcal O (n \exp (k))\quad \quad \text{where $n = |N|$ and $k$ is width}
\end{equation}

\subsubsection{Any Space Recursive Cut Set}
\begin{table}[H]
\centering
\begin{tabular}{l|l|l|c} \label{tab:anytime comparison}
      & no cache                           & all cache                 & $\Delta\, no \rightarrow all$ \\ \hline
space & $\mathcal O (wn)$                  & $\mathcal O (n \exp (w))$ & $\uparrow$       \\
time  & $\mathcal O (n \exp (w \log n )))$ & $\mathcal O (n \exp (w))$ & $\downarrow$    
\end{tabular}
\end{table}


\section{Compiling Bayesian Networks}
\subsection{Network Polynomials}
The network polynomial is a summation over all instantiations of a network, 
\begin{equation}
	f \triangleq \sum_z \prod_{\theta_{x | u} \sim z }\theta _{x | u} \prod _{\lambda _x \sim z} \lambda_x
\end{equation}

\subsection{AC Properties}
\paragraph{AC Size.}
of an AC is defined as the number of edges in the circuit.  
\paragraph{AC Complexity.}
is the size of smallest AC that represents the network polynomial. 

\paragraph{Decomposable.}
At each $\star$ node, we need 
\begin{equation}
	vars(AC_A) \cap vars(AC_B) = \emptyset
\end{equation}

\paragraph{Deterministic.}
At each $+$ node, we require at most one positive input is non-zero for all \textit{complete instantiation}. 

\paragraph{Smooth.}
At each $+$ node, we require 
\begin{equation}
	vars(AC_A) = vars(AC_B)
\end{equation}

\paragraph{AC for Marginals.} requires decomposable and smooth. This guarantees that sub-circuits are of complete variable instantiations. 

\paragraph{AC for Marginals and MPE.} requires all three above: decomposable, deterministic, and smooth. The additional determinism guarantees a 1-to-1 mapping between sub-circuits and complete variable instantiations. 

\subsection{AC Derivative Probabilistic Implications}
\begin{equation}
	\frac{\partial f}{\partial \lambda _\bx} (\be) = \pr( \bx, \be - X) 
\end{equation}
and 
\begin{equation}
	\theta_{\bx | \bu} \frac{\partial f}{\partial \theta_{\bx | \bu}} ( \be) = \pr (\bx, \bu, \be)
\end{equation}

\subsection{Compilation via Variable Elimination}
\paragraph{Circuit Factors.}
``In a circuit factor, each variable instantiation is mapped to a circuit node instead of a number.''

\paragraph{Operations.}
We use $+(n_1, n_2)$ to denote an addition node that has $n_1$ and $n_2$ as its children. Similarly, $\star (n_1, n_2)$ denotes a multiplication node. An operation (multiplication or addition) of two circuit factors $f(X)$ and $f(Y)$ is a factor over variables $Z = X \cup Y$, 
\begin{equation}
	f(z) = [\star \text{ or } +]( f(x), f(y)), \quad \text{ where } x \sim z \quad \text{and} \quad y \sim z 
\end{equation}

\paragraph{Procedure.}
\begin{enumerate}
	\item \textbf{Made nodes for each CPT.} For each family $X|U$, construct nodes $\star (\lambda_x, \theta_{x | u} )$ for each instantiation $xu$ of $XU$. 
	\item \textbf{Eliminate Everything.} We apply VE to eliminate all variables in the network to reach trivial instantiation $\top$ (corresponds to root). 
\end{enumerate}


\section{Causality - Interventions}
\subsection{Notations}
\paragraph{Causal Effect (CE).} of $X = x$ on $Y = y$ can be written as
\begin{equation}
	\pr (Y = y | do(X = x)) \equiv \pr (y | do (x)) \equiv \pr(y_x)
\end{equation}

\paragraph{Interventional Distribution.} For $\pr (X, Y, Z)$, the interventional distribution for $do(X = x)$ is denoted as 
\begin{equation}
	\pr _{X = x} (Y, Z)
\end{equation}


\subsection{Types of Causal Graphs}\footnote{Hidden variables are roots.}
\subsubsection{Markovian Model}
Each hidden variable in a Markovian Model has at most one child. It has an alternative name of ``no hidden confounders''. In this case, causal effects are always identifiable. 

\subsubsection{Semi-Markovian Model}
Some hidden variable has more than one child. In this case, causal effects are not always identifiable. 

\subsection{Identifiability Criterion}
\subsubsection{Causal Effect Rule}
The Causal Effect Rule links together association and intervention. It states the following: if $\mathbf Z$ are the parents of $X$, then
\begin{equation}
	\pr (y | do(x)) = \pr(y_x) = \sum_{\mathbf z} \pr (y | x, \bz ) \pr (\bz)
\end{equation}
The catch to this formulation is one have to know the parents - meaning that we need to have a correct causal structure prior to using this formula. This is a strong assumption. Often, the structure is exactly what we are after.\footnote{
Recall that different causal structures can generate the same distribution, and data alone is not enough.
} 

\subsubsection{Backdoor Criteria}
A path between $X$ and $Y$ is \textit{\color{Thistle} blocked} by $Z$ iff 
\begin{itemize}
	\item some collider is not in $Z$, or
	\item some non-collider is in $Z$. 
\end{itemize}
where a collider node is simply a convergent valve defined earlier ($\rightarrow W \leftarrow$). Here we distinguish only between colliders and non-colliders. The Backdoor Criteria states the following: Consider a causal graph $G$ and causal effect $\pr(y_x)$. A set of variables $\mathbf Z$ satisfis the backdoor criteria iff 
\begin{itemize}
	\item no node in $\mathbf Z$ is a descendant of $X$, 
	\item $\mathbf Z$ \textit{\color{Thistle} blocks} every path between $X$ and $Y$ that contains an arrow into $X$. 
\end{itemize}
Then, if $\mathbf Z$ is a backdoor, then
\begin{equation}
	\pr ( y _ x) = \sum_{\bz} \pr (y | x, \bz) \pr (\bz)
\end{equation}

\paragraph{Incompleteness.} The backdoor criteria is incomplete. When it identifies that a causal effect has no backdoor, the causal effect can be either identifiable or not identifiable (inconclusive).  


\subsubsection{Frontdoor Criteria}
Consider a causal graph $G$ and causal effect $\pr(y_x)$. A set of variables $\mathbf Z$ satisfies the frontdoor criteria iff \dots. Then if $\mathbf Z$ is a frontdoor, then, 
\begin{equation}
	\pr ( y _ x ) = \sum _ \bz \pr (\bz | x ) \sum_{x'} \pr(y | x', \bz ) \pr(x')
\end{equation}

\subsubsection{Exogenous X\label{sec:exox}}
For the query $\pr (y_x)$, when $X$ is an exogenous variable, meaning that it has no parents, 
\begin{equation}
	\pr(y _x) = \pr ( y | x)
\end{equation}

\subsection{The Do-Calculus}
The key idea is to apply a series of rules until we get a formula that is comprised of solely associational quantities. There are three re-write rules in total. $\mathbf X, \mathbf Y, \mathbf Z, \mathbf W$ are disjoint sets of variables, 
\paragraph{Rule 1. Ignoring Observations.}
\begin{equation}
	\pr (\by | do(\bx), \bz, \bw) = \pr (\by | do(\bx) , \bw) \quad \text{if} \quad \dsep_{G_{\overbar{\mathbf X}}} (\mathbf Y, \mathbf {XW}, \mathbf Z)
\end{equation}
where we perform $\dsep$ test on an altered graph $G_{\largeover{\mathbf x}}$, rather than the original causal graph $G$. (Detailed in Section \ref{sec:graph alteration}).

\paragraph{Rule 2. Action / Observation Exchange.}
\begin{equation}
	\pr (\by | do(\bx), do (\bz), \bw) = \pr (\by | do(\bx) ,\bz, \bw) \quad \text{if} \quad \dsep_{G_{\largeover {\mathbf X} \underbar{\mathbf Z}}} (\mathbf Y, \mathbf {XW}, \mathbf Z)
\end{equation}
There are several weakened versions of this rule. First, we consider the case of $\mathbf X = \emptyset$, then
\begin{equation}
	\pr (\by | do (\bz), \bw) = \pr (\by | \bz, \bw) \quad \text{if} \quad \dsep_{G_{\underbar{\mathbf Z}}} (\mathbf Y, \mathbf {W}, \mathbf Z)
\end{equation}
Simplifying even further, we can remove the common ``kept'' part $\mathbf W$, and get
\begin{equation}
		\pr (\by | do (\bz)) = \pr (\by | \bz) \quad \text{if} \quad \dsep_{G_{\underbar{\mathbf Z}}} (\mathbf Y, \emptyset, \mathbf Z)
\end{equation}
notice that this Exogenous X rule (also stated in Section \ref{sec:exox}) follows directly as a corollary of Rule 2. 


\paragraph{Rule 3. Ignoring Actions.}
\begin{equation}
	\pr (\by | do(\bx), do (\bz), \bw) = \pr (\by | do(\bx) , \bw) \quad \text{if} \quad \dsep_{G_{\largeover{\mathbf X} \largeover{Z(W)}}} (\mathbf Y, \mathbf {XW}, \mathbf Z)
\end{equation}
where encounter a special notation $\largeover{Z(W)}$ that means ``not all variables in $\mathbf Z$, but only those variables in $\mathbf Z$ that do not have ancestor in $\mathbf W$''. 

\subsubsection{Graph Alterations}\label{sec:graph alteration} The calculus rules we specified earlier performs dsep tests on altered graphs, where
\begin{itemize}
	\item $G_{\largeover{\mathbf x}}$ is obtained via removing edges pointing into variables $\mathbf X$ from $G$.
	\item $G_{\underbar{\mathbf x}}$ is obtained via removing edges pointing away from variables $\mathbf X$ from $G$.
\end{itemize}

\section{Counterfactual Reasoning}
Causal effects are on the second level: interventions. In counterfactual reasoning, we step up onto level three: what-if's. 

\subsection{The Information Hierarchy}
To add to what we have said above, 
\begin{itemize}
	\item Associational reasoning: Bayesian Network
	\item Interventional reasoning: Causal Bayesian network (causal graph)
	\item Counterfactual reasoning: Functional Bayesian network (functional dependencies)
\end{itemize}

\subsection{Counterfactual Queries}
\subsubsection{Probability of Necessity (PN)}
Probability that $y$ would not have occurred in the absence of $x$ ($do(\overbar x)$), given that $x$ and $y$ did in fact occur, i.e, 
\begin{equation}
	PN = \pr (\overbar y_{\overbar x} | x, y) 
\end{equation}

\subsubsection{Probability of Sufficiency (PS)}
Probability that setting $x$ would produce $y$ in a situation where both $x$ and $y$ are absent, i.e., 
\begin{equation}
	PS = \pr (y_x | \overbar y, \overbar x)
\end{equation}

\subsubsection{Probability of Necessity and Sufficiency (PNS)}
Probability that $y$ responds to $x$ both ways (measures the necessity and sufficiency for $x$ to produce $y$): 
\begin{equation}
	PNS = \pr (y_x, \overbar y_{\overbar x} ) = \pr (x, y) PN + \pr (\overbar x, \overbar y) PS
\end{equation}

\subsubsection{Probability of Disablement} Probability that $y$ would have been prevented if it were not for $x$, 
\begin{equation}
	PD = \pr (\overbar y _ {\overbar x} | y )
\end{equation}

\subsubsection{Probability of Enablement} Probability that $y$ would have been realized if it were not for absence of $x$, 
\begin{equation}
	PE = \pr (y_x | \overbar y) 
\end{equation}


\subsection{Structural Causal Models (SCM)}

\paragraph{World.} In SCM, a world is defined slightly different from what we had in associational graphs. A world is an instantiation of exogenous variables. This is because fixing exogenous variables fully specifies the entire network. 

\subsection{Evenets}
\subsubsection{Observational Event}
\begin{itemize}
	\item Form: $\bx$ where $\mathbf X$ is a set of endogenous variables
	\item Meaning: variables $\mathbf X$ took the value $\bx$
	\item Examples: $x$, \quad $\overbar y, z$
\end{itemize}

\subsubsection{Interventional Event}
\begin{itemize}
	\item Form: $\mathbf y _ \mathbf x$ where $\mathbf X$ and $\mathbf Y $ are sets of endogenous variables
	\item Meaning: variables $\mathbf Y$ took the value $\by$ after setting $\mathbf X = \bx$
	\item Examples: $y_x$, \quad $y_{\overbar x z}$, \quad $(x\overbar y)_{zw}$
\end{itemize}

\subsubsection{Counterfactual Event}
\begin{itemize}
	\item Form: $\eta_1, ..., \eta_n$ where $\eta_i$ is an observational or interventional event
	\item Meaning: $\bigwedge_i \eta_ i$
	\item Examples: mix of above two event examples
\end{itemize}

\subsection{Satisfaction}
Recalling what we had earlier, we denote $\bu$ satisfies event $\eta$ in SCM $\mathcal M$ as 
\begin{equation}
	\mathbf u \vDash_{\mathcal M} \eta
\end{equation}

\subsubsection{Observational Event}
We write 
\begin{equation}
	\bu \vDash_{\mathcal M} \mathbf x
\end{equation}
iff world $\bu$ fixes (endogenous) variables $\mathbf X$ to $\bx$ in SCM $\mathcal M$. 

\subsubsection{Interventional Event}
\begin{equation}
	\bu \vDash_{\mathcal M} \by_\bx \iff \bu \vDash_{\mathcal M_\bx} \by
\end{equation}

\subsubsection{Counterfactual Event}
\begin{equation}
	\bu \vDash_{\mathcal M} \eta_1, ..., \eta_n \iff \bigwedge_i \by \vDash_{\mathcal M} \eta_i
\end{equation}

\subsection{Exogeneity and Monotonicity}
\paragraph{Exogeneity.} $X$ is exogenous relative to $Y$ iff events $y_x$ and $x$ are independent and so are $y_{\overbar x}$ and $x$,
\begin{equation}
	\pr (y_x | x) = \pr (y_x) \quad \text{and} \quad \pr (y_{\overbar x}| x) = \pr ( y_{\overbar x}) 
\end{equation}

\paragraph{Monotonicity.} $Y$ is monotonic relative to $X$ iff the event $\overbar y _ x, y_{\overbar x}$ is unsatisfiable. This notion is general, $x$ and $y$ does not have to be inside the same family. A more general notation is to condition on probability of event $\overbar y _ x, y_{\overbar x}$ being zero. 












\end{document}
