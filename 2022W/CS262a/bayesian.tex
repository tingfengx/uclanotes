\documentclass[11pt]{article}

\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[
    xetex, 
    dvipsnames
]{xcolor}
\usepackage[
    colorlinks=true,
    linkcolor=black,
    urlcolor=Thistle
]{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{xargs}
\usepackage{ccicons}
\usepackage{mdframed}
\usepackage{caption}
\usepackage{cancel}
\usepackage{float}
\usepackage[nottoc]{tocbibind}
%\usepackage[
%    outputdir=.texpadtmp
%]{minted}

% ==== License =====
\usepackage[
    type={CC}, 
    modifier={by-nc-sa}, 
    version={4.0},
]{doclicense}

% ==== set font ====
\usepackage{amsmath}
\usepackage{unicode-math}
%\setmainfont{texgyrepagella-regular.otf}
\setmainfont{Palatino}
\setmathfont{texgyrepagella-math.otf}

% ==== todo notes ====
\usepackage[
    colorinlistoftodos,
    prependcaption,
    textsize=tiny
]{todonotes}
\newcommandx{\note}[2][1=]{\todo[linecolor=Thistle,backgroundcolor=Thistle!25,bordercolor=Thistle,#1]{#2}}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}

% General
\newcommand{\mc}[1]{\mathcal{#1}}

% Math Bold Font, Vector Notations
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bzero}{\mathbf{0}}

% Proofs, Structures
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\qed}{\hfill $\blacksquare$}

% Number Spaces, Vector Space
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\largeover}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\mathrm{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\independent}{\perp \!\!\! \perp}

\newcommand{\mods}{\mathrm{Mods}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\ent}{\mathrm{ENT}}
\newcommand{\mi}{\mathrm{MI}}
\newcommand{\dsep}{\mathrm{dsep}}

% Set section number in front of equation enumerations
\counterwithin{equation}{section}
\counterwithin{footnote}{section}
\author{\ccLogo\, Tingfeng Xia @ UCLA}
\title{CS262a Bayesian Networks}
\date{Winter Quarter, 2022}

\begin{document}
\maketitle
\doclicenseThis

\section*{Preface}
This document is intentionally kept short, and is not a complete summary of what is covered in this course. Excerpt from online: 
\begin{quotation}
\noindent The objective of this class is to provide an in-depth exposition of knowledge representation, reasoning, and machine learning under uncertainty using the framework of Bayesian networks. Both theoretical underpinnings and practical considerations will be covered, with a special emphasis on constructing and learning graphical models, and on various exact and approximate inference algorithms. Additional topics include logical approaches to probabilistic inference, compilation techniques, sensitivity analysis, undirected graphical models, and statistical relational learning.
\end{quotation}

\paragraph{Instructor:} \href{https://web.cs.ucla.edu/~darwiche/}{Professor Adnan Darwiche}
\paragraph{Book:} \href{https://www.cambridge.org/us/catalogue/catalogue.asp?isbn=9780521884389}{Adnan Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press 2009.}

\tableofcontents

\newpage
\section{Propositional Logic}
\subsection{Principle Logical Forms}
\paragraph{Inconsistent.} Something that never holds; $\mods (\cdot) = \empty$; $\pr(\alpha) = 0$
\paragraph{Valid.} Something that always holds; $\mods (\cdot) = \Omega$; $\pr (\alpha) = 1$
\paragraph{Equivalent.} $\mods (\alpha) = \mods (\beta)$
\paragraph{Mutual Exclusive.} $\mods(\alpha) \cap \mods (\beta) = \emptyset$
\paragraph{Exhaustive.} $\mods (\alpha) \cup \mods (\beta) = \Omega$
\paragraph{Entailment / Implication.} $\alpha \vDash \beta \triangleq \mods (\alpha) \subseteq \mods ( \beta)$

\subsection{Equivalent Forms}
\begin{itemize}
\item $\mods( \alpha \land \beta) = \mods( \alpha) \cap \mods(\beta)$
\item $\mods( \alpha \vee \beta) = \mods ( \alpha) \cup \mods ( \beta)$
\item $\mods ( \neg \alpha) = \largeover{\mods ( \alpha)}$
\end{itemize}

\subsection{Instantiation Agreement}
Two instantiation, each of which can cover a subset of different varaibles, are said to be compatible with each other if they argree on all common variables. Denoted as $\bx \sim \by$.

\subsection{Information Theory}
\paragraph{Entropy.} 
\begin{equation}
\ent  (X) = - \sum_x \pr (x) \log \pr( x)
\end{equation}
where $0 \log 0 = 0 $ by convention. With a higher entropy, we say that it is more chaotic. 

\paragraph{Conditional Entropy. }
\begin{equation}
	\ent (X | Y ) = \sum_y \pr (y) \ent(X | y) \quad \text{where} \quad \ent (X|y) = - \sum_{x} \pr(x|y) \log \pr(x | y)
\end{equation}
Conditioning never increases the entropy, i.e. 
\begin{equation}
	\ent (X | Y) \leq \ent (X)
\end{equation}

\paragraph{Mutual Information} 
\begin{align}
	\mi(X; Y) 
	&= \sum_{x, y} \pr (x, y) \log \frac{\pr (x, y)}{\pr (x) \pr (y)} \\
	&= \ent (X) - \ent (X | Y) \\
	&= \ent (Y) - \ent ( Y | X) 
\end{align}

\paragraph{Conditional Mutual Information}
\begin{align}
	\mi ( X; Y | Z) 
	&= \sum_{x, y, z} \pr (x, y , z) \log \frac{\pr (x, y | z)}{\pr (x | z) \pr(y | z)} \\
	&= \ent (X | Z) - \ent (X | Y, Z) \\
	&= \ent (Y | Z) - \ent (Y | X, Z)
\end{align}

\section{Probability Calculus}
\subsection{Bayesian Conditioning}
Bayesian Condition is specified by the formula 
\begin{equation}
	\pr ( \alpha | \beta) = \frac{\pr ( \alpha \land \beta )}{\pr ( \beta) }
\end{equation}
In particular, not to be confused with Bayesian inference (to be added later). 

\subsection{Independence and Notations}
\paragraph{Independence.}  
\begin{align}
	\alpha \independent \beta 
	& \iff \pr ( \alpha | \beta) = \pr ( \alpha) \vee \pr( \beta) = 0 \\
	& \iff \pr ( \alpha \land \beta) = \pr(\alpha) \pr ( \beta)
\end{align}

\paragraph{Conditional Independence.}
\begin{align}
	(\alpha \independent \beta) | \gamma 
	&\iff \pr ( \alpha | \beta \land \gamma) = \pr ( \alpha | \gamma) \vee \pr(\beta \land \gamma) = 0 \\
	&\iff \pr ( \alpha \land \beta | \gamma) = \pr ( \alpha | \gamma ) \pr(\beta | \gamma ) \vee \pr ( \gamma) = 1
\end{align}

\paragraph{Set Independence}
\begin{equation}
	I_{\pr} (X, Z, Y) \iff (x \independent y) | z, \quad \forall x, y, z \in X, Y, Z
\end{equation}

\section{Bayesian Networks}
\subsection{Soft Evidence}
\subsubsection{All Things Considered Method}
We normalize / rescale $w$ according to new evidence. 
\begin{align}
\pr' (w) = \begin{cases}
	\frac{\pr ' (\beta)}{\pr (\beta)} \pr ( w) &  \text{if} w \vDash \beta \\
	\frac{\pr' (\neg \beta)}{\pr (\neg \beta)} \pr (w) & \text{if} w \vDash \neg \beta
\end{cases}
\end{align}

The closed form is called the Jefferey's Rule. 

\paragraph{Jeffery's Rule}
\begin{equation}
	\pr ' (\alpha) = q \pr ( \alpha | \beta ) + (1 - q) \pr ( \alpha | \neg \beta)
\end{equation}

\paragraph{Jeffery's Rule - General Case}
\begin{equation}
	\pr'(\alpha) = \sum_{i = 1}^n\pr ' (\beta_i) \pr( \alpha | \beta_i) 
\end{equation}

\subsubsection{Nothing-else Considered Method}
\paragraph{Odds. } 
\begin{equation}
	O(\beta) = \frac{\pr( \beta)}{\pr ( \neg \beta)}
\end{equation}

\paragraph{Bayes Factor} 
\begin{equation}
	k = \frac{O'(\beta)}{O ( \beta)} = \frac{Pr'(\beta) / \pr'(\neg \beta)}{...}
\end{equation}
from where we can expand and organize
\begin{equation}
	\pr' ( \beta) = \frac{k \pr ( \beta) }{k \pr ( \beta) + \pr ( \neg \beta) } 
\end{equation}

\paragraph{Closed Form Solution.}
\begin{equation}
	\pr' (\alpha) = \frac{k\pr ( \alpha \land \beta) + \pr (\alpha \land \neg \beta)}{k \pr ( \beta ) + \pr ( \neg \beta)}
\end{equation}

\subsection{Noisy Sensors}
\begin{equation}
	O'(\beta) = \underbrace{\frac{1 - f_n}{f_p}}_{k^+} O(\beta) \quad \quad O'(\beta) = \underbrace{\frac{f_n }{1 - f_p}}_{k^-} O(\beta)
\end{equation}

\subsection{Markov Assumptions}
\begin{equation}
	\mathrm{Markov}(G) = \{I_\pr ( V, \mathrm {Parents} (V), \mathrm{ND} (V) \}_V
\end{equation}
where $\mathrm{ND}$ means non-descendants, and includes all nodes except for $V, \mathrm {Parents} (V)$ and $\mathrm {Descendants}(V)$ (all the way till leaf)

\subsection{Graphoid Axioms\label{sec:graphoid}}
\paragraph{Symmetry.}
\begin{equation}
	I_\pr (X, Z, Y) \iff I _\pr (Y, Z, X) 
\end{equation}

\paragraph{Decomposition.}
\begin{equation}
	I_\pr (X, Z, Y \cup W) \implies I _\pr (X, Z, Y) \land I _\pr (X, Z, W)
\end{equation}

\paragraph{Weak Union.}
\begin{equation}
	I_\pr (X, Z, Y \cup W) \implies I_\pr (X, Z \cup Y, W) 
\end{equation}

\paragraph{Contraction.}
\begin{equation}
	I_\pr (X, Z, Y) \land I_\pr (X, Z \cup Y, W ) \implies I_\pr (X, Z, Y \cup W )
\end{equation}

\paragraph{Triviality.}
\begin{equation}
	I_\pr (X, Z, \emptyset) 
\end{equation}

\subsection{Positive Graphoid Axioms}
... includes everything from Graphoid Axioms (Section \ref{sec:graphoid}) and in addition has 
\paragraph{Intersection.}
\begin{equation}
	I _ \pr (X, Z \cup W, Y ) \land I _\pr (X, Z \cup Y , W ) \implies I _\pr (X, Z, Y \cup W)
\end{equation}

\subsection{D-seperation Linear Prune Theorem}

\subsection{D-seperation Properties}
\paragraph{Soundness.} 
\begin{equation}
	\dsep_ G ( X, Z, Y ) \implies I _\pr (X, Z, Y)
\end{equation}

\paragraph{(Weak) Completeness.}
There exists a parametrization $\Theta$ that for every DAG $G$ such that
\begin{equation}
	I_\pr (X, Z, Y) \iff \dsep_G (X, Z, Y)
\end{equation}

\section{Inference by Factor Elimination}
\subsection{Elimination Trees}
\paragraph{Variables.}
$vars(i)$ denotes the variables mentioned at node $i$. $vars(i, j)$ denotes all variables mentioned in nodes to the $i$-side of the graph (inclusive). Hence, it holds that $vars(i) \subseteq vars(i, j)$.

\paragraph{Separators.} 
\begin{equation}
	S_{ij} \triangleq vars(i, j) \cap vars(j, i)
\end{equation}

\paragraph{Clusters.} 
\begin{equation}
	C_i \triangleq vars(i) \cup \bigcup_j S_{ij}
\end{equation}

\section{Inference by Conditioning}
\subsection{Run time Comparison}
\subsubsection{Variable Elimination (VE)}
Let $w$ be the width of the tree, $n$ denote the number of variables, and $|Q|$ as query variable size.
\paragraph{Time Complexity.} $\mathcal O (n \exp (w))$

\paragraph{Space Complexity.} 
\begin{equation}
	\mathcal O (n \exp (w) + n \exp (|Q|)) \equiv \mathcal O (n \exp (w)) \text{, \quad if } |Q| < \infty
\end{equation}

\subsubsection{Massage Passing}
\paragraph{One Specific Message.} The cost to pass one specific message is
\begin{equation}
	\mathcal O (\exp ( |C_i|)) 
\end{equation}
where $|C_i|$ is the cluster size. 

\paragraph{Every Message.} Since cluster sizes are bounded above by tree width, we can say that passing of every message will have an upper-bound runtime of
\begin{equation}
	\mathcal O ( \exp (w) )
\end{equation}
where $w$ is the width of the elimination tree. 

\paragraph{Amount of Messages.} The total amount of messages is 
\begin{equation}
	\mathcal O (2 (m - 1)) \quad \quad \text{where $m = |V|$}
\end{equation}
since we have a tree structure, meaning we have exactly $(m - 1)$ edges and each edge can have forward backward each once. 

\paragraph{All Messages - All Cluster Marginals.} The total time to pass all messages and compute all cluster marginals is 
\begin{equation}
	\mathcal O (m \exp (w)) \quad \quad \text{or} \quad \quad \mathcal O (n \exp( w))\text{, for $\mathcal O (n)$ edges.} 
\end{equation}

\subsubsection{Polytree / Belief Propagation}
\paragraph{Runtime.} Define $k$ as the max number of parents in the poly tree, then $k$ is the same as the width of elimination tree. Let, also, $n$ denote the number of nodes in the polytree. The algorithm has runtime 
\begin{equation}
	\mathcal O (n \exp (k))
\end{equation}

\subsubsection{Cut Set Conditioning}
\paragraph{Time and Space (Total) Complexity.}
\begin{equation}
	\mathcal O (n \exp (k))\quad \quad \text{where $n = |N|$ and $k$ is width}
\end{equation}

\subsubsection{Any Space Recursive Cut Set}
\begin{table}[H]
\centering
\begin{tabular}{l|l|l|c} \label{tab:anytime comparison}
      & no cache                           & all cache                 & $\Delta\, no \rightarrow all$ \\ \hline
space & $\mathcal O (wn)$                  & $\mathcal O (n \exp (w))$ & $\uparrow$       \\
time  & $\mathcal O (n \exp (w \log n )))$ & $\mathcal O (n \exp (w))$ & $\downarrow$    
\end{tabular}
\end{table}


\section{Compiling Bayesian Networks}
\subsection{Network Polynomials}
The network polynomial is a summation over all instantiations of a network, 
\begin{equation}
	f \triangleq \sum_z \prod_{\theta_{x | u} \sim z }\theta _{x | u} \prod _{\lambda _x \sim z} \lambda_x
\end{equation}

\subsection{AC Properties}
\paragraph{AC Size.}
of an AC is defined as the number of edges in the circuit.  
\paragraph{AC Complexity.}
is the size of smallest AC that represents the network polynomial. 

\paragraph{Decomposable.}
At each $\star$ node, we need 
\begin{equation}
	vars(AC_A) \cap vars(AC_B) = \emptyset
\end{equation}

\paragraph{Deterministic.}
At each $+$ node, we require at most one positive input is non-zero for all \textit{complete instantiation}. 

\paragraph{Smooth.}
At each $+$ node, we require 
\begin{equation}
	vars(AC_A) = vars(AC_B)
\end{equation}

\paragraph{AC for Marginals.} requires decomposable and smooth. This guarantees that sub-circuits are of complete variable instantiations. 

\paragraph{AC for Marginals and MPE.} requires all three above: decomposable, deterministic, and smooth. The additional determinism guarantees a 1-to-1 mapping between sub-circuits and complete variable instantiations. 


\subsection{AC Derivative Probabilistic Implications}
\begin{equation}
	\frac{\partial f}{\partial \lambda _\bx} (\be) = \pr( \bx, \be - X) 
\end{equation}
and 
\begin{equation}
	\theta_{\bx | \bu} \frac{\partial f}{\partial \theta_{\bx | \bu}} ( \be) = \pr (\bx, \bu, \be)
\end{equation}













\end{document}
